# CLAUDE.md - Инструкции для Claude Code

## Обзор проекта

**Log Explorer MCP** — это MCP сервер для интеллектуального анализа логов через LLM. Ключевая идея: логи слишком большие для контекста LLM, поэтому мы используем кластеризацию для сжатия информации и lazy exploration для постепенного углубления в детали.

## Концепция

### Проблема
- Логи содержат миллионы строк — не влезают в контекст
- Grep требует знать что искать заранее
- Формат логов неизвестен до анализа

### Решение
1. **Кластеризация по похожести** — группируем похожие строки, извлекаем шаблоны
2. **Lazy exploration** — сначала статистика и примеры, детали по запросу
3. **LLM-driven drill-down** — LLM сама решает куда углубляться
4. **Временной анализ** — динамика логов, детекция аномалий

### Алгоритм similarity (ключевая инновация)

Токен-based извлечение шаблонов:
```
"User john logged in from 192.168.1.1"
"User admin logged in from 10.0.0.5"
         ↓
"User .* logged in from .*"
```

Алгоритм в `clustering.js`:
1. Токенизация (слова, числа, пунктуация отдельно)
2. LCS на уровне токенов (не символов!)
3. Жадный выбор непересекающихся блоков
4. Формирование шаблона с `.*` на месте различий
5. Similarity = 2 × matched_len / (len_a + len_b)

## Структура проекта

```
log-explorer-mcp/
├── CLAUDE.md          # Этот файл - инструкции для Claude Code
├── README.md          # Документация для пользователей
├── ARCHITECTURE.md    # Детальная архитектура
├── TODO.md            # Задачи на развитие
├── package.json       # Node.js конфигурация (ESM)
├── server.js          # MCP сервер - точка входа
├── clustering.js      # Алгоритм кластеризации
├── timestamps.js      # Парсинг временных меток
├── test-cli.js        # CLI для тестирования без MCP
└── generate-test-logs.cjs  # Генератор тестовых данных
```

## Как запускать

```bash
# Установка зависимостей
npm install

# Тестирование CLI (без MCP)
node generate-test-logs.cjs /tmp/test.log
node test-cli.js /tmp/test.log

# Запуск MCP сервера
node server.js
```

## MCP Tools (API)

| Tool | Назначение | Когда использовать |
|------|-----------|-------------------|
| `log_overview` | Общая информация о файле | Первый шаг анализа |
| `log_cluster` | Кластеризация с шаблонами | Понять структуру логов |
| `log_cluster_drill` | Подкластеры внутри кластера | Углубиться в интересный кластер |
| `log_timeline` | Временная гистограмма | Найти аномалии по времени |
| `log_grep` | Поиск (count + примеры) | Проверить гипотезу |
| `log_fetch` | Сырые строки | Когда точно знаешь что нужно |

## Принципы разработки

1. **Lazy by default** — минимум данных на каждом шаге
2. **Statistics first** — сначала count/histogram, потом примеры
3. **No Python** — проект на Node.js (предпочтение владельца)
4. **Простота алгоритмов** — без ML/embeddings, чистые алгоритмы

## Текущее состояние

✅ Реализовано:
- Базовая кластеризация с извлечением шаблонов
- Парсинг временных меток (несколько форматов)
- Временные гистограммы с детекцией аномалий
- MCP сервер со всеми tools
- CLI для тестирования

⚠️ Требует доработки:
- См. TODO.md для списка задач

## Команды для разработки

```bash
# Тест кластеризации на реальных логах
node test-cli.js /var/log/syslog

# Генерация логов с аномалиями
node generate-test-logs.cjs /tmp/anomaly.log

# Проверка MCP протокола (stdin/stdout)
echo '{"jsonrpc":"2.0","method":"tools/list","id":1}' | node server.js
```

## Контекст создания

Проект создан в диалоге о анализе логов через LLM. Исходная идея: кластеризация логов позволяет LLM понять структуру без чтения всех строк, а затем интерактивно "нырять" в интересные кластеры.

Вдохновение: алгоритм Drain для log parsing, но с фокусом на interactive exploration вместо batch processing.
